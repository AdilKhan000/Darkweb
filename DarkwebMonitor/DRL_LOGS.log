INFO 2024-02-26 18:05:48,900: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:05:48,903: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:05:48,905: Enabled addons:
[]
DEBUG 2024-02-26 18:05:48,909: Using selector: EpollSelector
DEBUG 2024-02-26 18:05:48,911: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:05:48,911: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:05:48,933: Telnet Password: b66af4238fd8ecb3
WARNING 2024-02-26 18:05:49,103: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:05:49,108: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:05:49,109: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
CRITICAL 2024-02-26 18:05:49,159: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:05:49,160: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:05:49,162: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
CRITICAL 2024-02-26 18:05:49,163: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
INFO 2024-02-26 18:13:20,378: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:13:20,381: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:13:20,383: Enabled addons:
[]
DEBUG 2024-02-26 18:13:20,384: Using selector: EpollSelector
DEBUG 2024-02-26 18:13:20,385: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:13:20,387: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:13:20,395: Telnet Password: 3240c22761583034
WARNING 2024-02-26 18:13:20,537: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:13:20,540: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:13:20,541: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
CRITICAL 2024-02-26 18:13:20,567: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:13:20,569: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:13:20,570: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
CRITICAL 2024-02-26 18:13:20,574: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
INFO 2024-02-26 18:15:43,016: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:15:43,019: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:15:43,021: Enabled addons:
[]
DEBUG 2024-02-26 18:15:43,023: Using selector: EpollSelector
DEBUG 2024-02-26 18:15:43,025: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:15:43,026: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:15:43,034: Telnet Password: a0ff59825de98f61
WARNING 2024-02-26 18:15:43,130: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:15:43,133: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:15:43,134: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
CRITICAL 2024-02-26 18:15:43,159: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:15:43,160: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:15:43,162: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
CRITICAL 2024-02-26 18:15:43,164: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
INFO 2024-02-26 18:23:04,821: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:23:04,823: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:23:04,826: Enabled addons:
[]
DEBUG 2024-02-26 18:23:04,827: Using selector: EpollSelector
DEBUG 2024-02-26 18:23:04,828: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:23:04,829: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:23:04,841: Telnet Password: f4f37a96bc2e0da6
WARNING 2024-02-26 18:23:04,937: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:23:04,940: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:23:04,941: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
CRITICAL 2024-02-26 18:23:04,967: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:23:04,968: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:23:04,970: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
CRITICAL 2024-02-26 18:23:04,976: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 79, in load_object
    mod = import_module(module)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'dark_web_scraping'
INFO 2024-02-26 18:26:11,012: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:26:11,015: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:26:11,018: Enabled addons:
[]
DEBUG 2024-02-26 18:26:11,019: Using selector: EpollSelector
DEBUG 2024-02-26 18:26:11,020: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:26:11,022: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:26:11,031: Telnet Password: d22b3137cdb7295e
WARNING 2024-02-26 18:26:11,134: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:26:11,137: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:26:11,138: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
CRITICAL 2024-02-26 18:26:11,184: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:26:11,185: Unhandled error in Deferred:
CRITICAL 2024-02-26 18:26:11,186: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 82, in load_object
    obj = getattr(mod, name)
AttributeError: module 'DarkwebMonitor.middlewares' has no attribute 'RandomUserAgentMiddleware'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 84, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'DarkwebMonitor.middlewares' doesn't define any object named 'RandomUserAgentMiddleware'
CRITICAL 2024-02-26 18:26:11,187: 
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 82, in load_object
    obj = getattr(mod, name)
AttributeError: module 'DarkwebMonitor.middlewares' has no attribute 'RandomUserAgentMiddleware'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/twisted/internet/defer.py", line 2000, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 158, in crawl
    self.engine = self._create_engine()
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/crawler.py", line 172, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/engine.py", line 99, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/__init__.py", line 97, in __init__
    DownloaderMiddlewareManager.from_crawler(crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 90, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/middleware.py", line 66, in from_settings
    mwcls = load_object(clspath)
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/utils/misc.py", line 84, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'DarkwebMonitor.middlewares' doesn't define any object named 'RandomUserAgentMiddleware'
INFO 2024-02-26 18:32:23,120: Scrapy 2.11.1 started (bot: DarkwebMonitor)
INFO 2024-02-26 18:32:23,123: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
INFO 2024-02-26 18:32:23,125: Enabled addons:
[]
DEBUG 2024-02-26 18:32:23,126: Using selector: EpollSelector
DEBUG 2024-02-26 18:32:23,127: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG 2024-02-26 18:32:23,128: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
INFO 2024-02-26 18:32:23,136: Telnet Password: 5a8cc00e97e61c90
WARNING 2024-02-26 18:32:23,245: /home/sydtalha/.local/lib/python3.10/site-packages/scrapy/extensions/feedexport.py:406: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

INFO 2024-02-26 18:32:23,248: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO 2024-02-26 18:32:23,249: Overridden settings:
{'BOT_NAME': 'DarkwebMonitor',
 'DEPTH_LIMIT': 5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'NEWSPIDER_MODULE': 'DarkwebMonitor.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['DarkwebMonitor.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
INFO 2024-02-26 18:32:23,428: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'DarkwebMonitor.middlewares.RandomUserAgentMiddleware',
 'DarkwebMonitor.middlewares.ProxyMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO 2024-02-26 18:32:23,434: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO 2024-02-26 18:32:23,458: Enabled item pipelines:
['DarkwebMonitor.pipelines.DarkwebmonitorPipeline']
INFO 2024-02-26 18:32:23,459: Spider opened
INFO 2024-02-26 18:32:23,466: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO 2024-02-26 18:32:23,468: Telnet console listening on 127.0.0.1:6023
DEBUG 2024-02-26 18:32:23,777: Retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/robots.txt> (failed 1 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:23,780: Retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/robots.txt> (failed 1 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:24,081: Retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/robots.txt> (failed 2 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:24,115: Retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/robots.txt> (failed 2 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:24,417: Gave up retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/robots.txt> (failed 3 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:24,418: Error downloading <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/robots.txt>: Connection was refused by other side: 111: Connection refused.
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:24,422: Gave up retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/robots.txt> (failed 3 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:24,423: Error downloading <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/robots.txt>: Connection was refused by other side: 111: Connection refused.
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:24,723: Retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/> (failed 1 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:24,726: Retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/> (failed 1 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:25,027: Retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/> (failed 2 times): Connection was refused by other side: 111: Connection refused.
DEBUG 2024-02-26 18:32:25,030: Retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/> (failed 2 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:25,333: Gave up retrying <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/> (failed 3 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:25,335: Gave up retrying <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/> (failed 3 times): Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:25,435: Error downloading <GET http://jgwe5cjqdbyvudjqskaajbfibfewew4pndx52dye7ug3mt3jimmktkid.onion/>
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.
ERROR 2024-02-26 18:32:25,472: Error downloading <GET http://6nhmgdpnyoljh5uzr5kwlatx2u3diou4ldeommfxjz3wkhalzgjqxzqd.onion/>
Traceback (most recent call last):
  File "/home/sydtalha/.local/lib/python3.10/site-packages/scrapy/core/downloader/middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.
INFO 2024-02-26 18:32:25,577: Closing spider (finished)
INFO 2024-02-26 18:32:25,640: Stored json feed (0 items) in: scraped_data_DRL_2024-02-26T13-02-23+00-00.json
INFO 2024-02-26 18:32:25,642: Dumping Scrapy stats:
{'downloader/exception_count': 12,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 12,
 'downloader/request_bytes': 3816,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'elapsed_time_seconds': 2.11328,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2024, 2, 26, 13, 2, 25, 579252, tzinfo=datetime.timezone.utc),
 'log_count/DEBUG': 11,
 'log_count/ERROR': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 147062784,
 'memusage/startup': 147062784,
 'retry/count': 8,
 'retry/max_reached': 4,
 'retry/reason_count/twisted.internet.error.ConnectionRefusedError': 8,
 "robotstxt/exception_count/<class 'twisted.internet.error.ConnectionRefusedError'>": 2,
 'robotstxt/request_count': 2,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2024, 2, 26, 13, 2, 23, 465972, tzinfo=datetime.timezone.utc)}
INFO 2024-02-26 18:32:25,642: Spider closed (finished)
